{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cred_f = 'C:\\Users\\Vihaan\\Documents\\UCB\\W210_Capstone\\data_Crerdible\\data\\Credible'\n",
    "ncred_f = 'C:\\\\Users\\\\Vihaan\\\\Documents\\\\UCB\\\\W210_Capstone\\\\data\\\\data\\\\notCredible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "modeling = pd.DataFrame(columns=('label', 'text', 'title'))\n",
    "i = 0    \n",
    "for root, dirs, files in os.walk(cred_f):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and 'api' not in file:\n",
    "             curr_file = os.path.join(root, file)\n",
    "             #print curr_file\n",
    "             with open(curr_file) as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    modeling.loc[i] = [0,data[\"text\"],data[\"title\"]]\n",
    "                    #data[\"url\"],data[\"date\"],data[\"source\"],data[\"text\"],data[\"images\"],data[\"videos\"]\n",
    "                    #print('')\n",
    "                    i = i + 1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "for root, dirs, files in os.walk(ncred_f):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and 'api' not in file:\n",
    "             curr_file = os.path.join(root, file)\n",
    "             #print curr_file\n",
    "             with open(curr_file) as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    modeling.loc[i] = [1,data[\"text\"],data[\"title\"]]\n",
    "                    #data[\"url\"],data[\"date\"],data[\"source\"],data[\"text\"],data[\"images\"],data[\"videos\"]\n",
    "                    #print('')\n",
    "                    i = i + 1\n",
    "                except ValueError:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>As Inauguration Day approached, Obama White Ho...</td>\n",
       "      <td>Obama Administration Rushed to Preserve Intell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1.0</td>\n",
       "      <td>MSNBC’s Mika Brzezinski just erased any linger...</td>\n",
       "      <td>MSNBC Just Admitted: “Our Job” Is To “Control ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Republican efforts to revise the Affordable Ca...</td>\n",
       "      <td>House GOP proposal to replace Obamacare sparks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>1.0</td>\n",
       "      <td>By Kurt Nimmo\\n\\nThe French establishment warn...</td>\n",
       "      <td>France Blames Russia for Rise of Marine Le Pen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  \\\n",
       "260    0.0  As Inauguration Day approached, Obama White Ho...   \n",
       "707    1.0  MSNBC’s Mika Brzezinski just erased any linger...   \n",
       "400    0.0  Republican efforts to revise the Affordable Ca...   \n",
       "621    1.0  By Kurt Nimmo\\n\\nThe French establishment warn...   \n",
       "\n",
       "                                                 title  \n",
       "260  Obama Administration Rushed to Preserve Intell...  \n",
       "707  MSNBC Just Admitted: “Our Job” Is To “Control ...  \n",
       "400  House GOP proposal to replace Obamacare sparks...  \n",
       "621  France Blames Russia for Rise of Marine Le Pen...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling = modeling.sample(frac=1) #shuffle\n",
    "modeling[1:5]\n",
    "#print modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape is  (815, 3)\n",
      "test data shape is (204, 3)\n",
      "Fraction of not credible news in train data 0.524\n",
      "Fraction of not credible news in test data 0.529\n"
     ]
    }
   ],
   "source": [
    "#Split into train, test sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train, test = train_test_split(modeling, test_size = 0.2)\n",
    "print \"train data shape is \", train.shape\n",
    "print \"test data shape is\", test.shape\n",
    "print \"Fraction of not credible news in train data\", round(train['label'].mean(),3)\n",
    "print \"Fraction of not credible news in test data\", round(test['label'].mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "#pipeline.fit(train['text'].values, train['label'].values)\n",
    "#pipeline.predict(test['text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Metrics\n",
      "('Total articles classified:', 815)\n",
      "('Accuracy Score:', 0.926)\n",
      "('F1 Score:', 0.926)\n",
      "Confusion matrix:\n",
      "[[374  14]\n",
      " [ 46 381]]\n"
     ]
    }
   ],
   "source": [
    "#Perform cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "k_fold = KFold(n=len(train), n_folds=6)\n",
    "scores = []\n",
    "f_scores=[]\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = train.iloc[train_indices]['text'].values\n",
    "    train_y = train.iloc[train_indices]['label'].values\n",
    "    #print train_text,train_y\n",
    "\n",
    "    test_text = train.iloc[test_indices]['text'].values\n",
    "    test_y = train.iloc[test_indices]['label'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    f_score = f1_score(test_y, predictions)\n",
    "    score = accuracy_score(test_y, predictions)\n",
    "    scores.append(score)\n",
    "    f_scores.append(f_score)\n",
    "    \n",
    "print 'Cross Validation Metrics'\n",
    "print('Total articles classified:', len(train))\n",
    "print('Accuracy Score:', round(sum(scores)/len(scores),3))\n",
    "print('F1 Score:', round(sum(f_scores)/len(f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Metrics\n",
      "('Total articles classified:', 204)\n",
      "('Accuracy Score:', 0.922)\n",
      "('F1 Score:', 0.922)\n",
      "Confusion matrix:\n",
      "[[94  2]\n",
      " [14 94]]\n"
     ]
    }
   ],
   "source": [
    "#Test on test dataset\n",
    "\n",
    "test_preds = pipeline.predict(test['text'])\n",
    "confusion = confusion_matrix(test['label'],test_preds)\n",
    "f_score = f1_score(test['label'], test_preds)\n",
    "score = accuracy_score(test['label'], test_preds)\n",
    "\n",
    "print 'Test Dataset Metrics'\n",
    "print('Total articles classified:', len(test))\n",
    "print('Accuracy Score:', round(score,3))\n",
    "print('F1 Score:', round(f_score,3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
